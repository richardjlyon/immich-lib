---
phase: 06-synthetic-integration-tests
plan: 01
type: execute
---

<objective>
Set up Docker test environment for Immich with fixture seeding capability.

Purpose: Create isolated Immich instance for integration testing with synthetic fixtures, validating that Immich correctly detects our test images as duplicates.
Output: Docker compose configuration, seed script, and bootstrap/teardown scripts.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-synthetic-integration-tests/06-04-CONTEXT.md
@.planning/phases/06-synthetic-integration-tests/06-03-03-SUMMARY.md
@tests/fixtures/MANIFEST.md

**Prior context:**
- 34 synthetic fixtures generated in tests/fixtures/ (w1-w8, c1-c8, f1-f7, x1-x11)
- Each scenario has manifest.json with expected_winner field
- Fixtures have controlled EXIF metadata for testing scoring algorithm
- Some fixtures (X6, X8) are partial due to encoding limitations

**Key insight from CONTEXT.md:**
- Start simple: clean bootstrap each time, no snapshots
- Core validation: Immich must recognize synthetic pairs as duplicates
- 30-60 second startup is acceptable
- Local-only, no CI automation yet

**Immich requirements (from Context7):**
- Docker Compose with: postgres, redis, immich-server, immich-machine-learning
- ML container required for CLIP-based duplicate detection
- CLI or API for uploading fixtures
- API key authentication for our tool to query duplicates
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Docker Compose configuration for test Immich</name>
  <files>tests/docker/docker-compose.yml, tests/docker/.env.test</files>
  <action>
Create minimal Docker Compose configuration for Immich test instance:

1. Create tests/docker/ directory structure
2. docker-compose.yml with services:
   - postgres:16-alpine with pgvector extension (immich requirement)
   - redis:alpine for caching
   - immich-server (ghcr.io/immich-app/immich-server:release)
   - immich-machine-learning (ghcr.io/immich-app/immich-machine-learning:release)

3. Configuration notes:
   - Use fixed port 2283 for server (standard Immich port)
   - Mount tests/fixtures as read-only volume for seeding
   - Use named volumes for postgres data and model cache
   - Set IMMICH_MACHINE_LEARNING_URL for server to find ML service

4. .env.test with:
   - DB_PASSWORD (test value)
   - UPLOAD_LOCATION (use named volume)
   - IMMICH_VERSION=release

Key: Keep it minimal - no hardware acceleration, no external storage, just enough to run duplicate detection.
  </action>
  <verify>docker compose -f tests/docker/docker-compose.yml config validates successfully</verify>
  <done>docker-compose.yml parses without errors, all required services defined</done>
</task>

<task type="auto">
  <name>Task 2: Create bootstrap and teardown scripts</name>
  <files>tests/docker/bootstrap.sh, tests/docker/teardown.sh, tests/docker/wait-for-ready.sh</files>
  <action>
Create shell scripts for environment lifecycle:

1. bootstrap.sh:
   - Check Docker is running
   - Clean any existing containers/volumes (ensure fresh state)
   - Start docker compose in detached mode
   - Call wait-for-ready.sh
   - Create admin user via API (POST /api/auth/admin-sign-up)
   - Generate and output API key for test use
   - Echo API key and base URL for downstream scripts

2. wait-for-ready.sh:
   - Poll GET /api/server/ping until healthy (max 120s timeout)
   - Check ML service is responding (GET /api/server/features shows ML enabled)
   - Exit 0 on success, exit 1 on timeout

3. teardown.sh:
   - docker compose down -v (remove volumes for clean slate)
   - Confirm containers stopped

Scripts should be idempotent - running bootstrap twice should result in clean state.
Use curl for HTTP calls. All scripts should be POSIX sh compatible.
  </action>
  <verify>
./tests/docker/bootstrap.sh runs and outputs API key
curl http://localhost:2283/api/server/ping returns {"res":"pong"}
./tests/docker/teardown.sh removes all containers
  </verify>
  <done>Can bootstrap fresh Immich, get API key, and tear down cleanly</done>
</task>

<task type="auto">
  <name>Task 3: Create fixture seeding script</name>
  <files>tests/docker/seed-fixtures.sh</files>
  <action>
Create script to upload all test fixtures to Immich:

1. seed-fixtures.sh:
   - Accept API_KEY and BASE_URL as arguments or env vars
   - For each scenario directory in tests/fixtures/:
     - Read manifest.json to get list of images
     - Upload each image via Immich API (POST /api/assets with multipart form)
     - Track uploaded asset IDs
   - After all uploads complete, trigger library scan (POST /api/libraries/scan if needed)
   - Wait for ML processing to complete (check job queue is empty)
   - Output summary: X scenarios, Y assets uploaded

2. Upload approach using curl multipart:
   ```
   curl -X POST "${BASE_URL}/api/assets" \
     -H "x-api-key: ${API_KEY}" \
     -F "assetData=@${file}" \
     -F "deviceAssetId=${scenario}_${filename}" \
     -F "deviceId=test-harness" \
     -F "fileCreatedAt=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
     -F "fileModifiedAt=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
     -F "isFavorite=false"
   ```

3. Wait for ML jobs to complete:
   - Poll GET /api/jobs until all queues show 0 active/waiting
   - This ensures duplicate detection has run on all uploads

Note: Skip non-image files in directories (like manifest.json).
Handle both .jpg and .mp4 files (X5 video scenario).
  </action>
  <verify>
./tests/docker/seed-fixtures.sh outputs "Uploaded X assets from Y scenarios"
curl with API key to /api/duplicates returns non-empty array after seeding
  </verify>
  <done>Fixtures upload successfully, Immich detects duplicates after ML processing</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete Docker test environment with Immich instance and seeded fixtures</what-built>
  <how-to-verify>
1. Run: cd tests/docker && ./bootstrap.sh
   - Should output API key and "Immich ready" message

2. Run: ./seed-fixtures.sh (using the output API key)
   - Should upload all 34 scenarios worth of fixtures

3. Visit: http://localhost:2283 in browser
   - Login with admin credentials from bootstrap
   - Navigate to "Duplicates" section in sidebar
   - Verify: Duplicate groups are detected and shown

4. Spot check one scenario:
   - Find a W1 (clear dimension winner) duplicate pair
   - Verify both images show in the duplicate view

5. Run: ./teardown.sh
   - Verify containers stop and volumes removed
  </how-to-verify>
  <resume-signal>Type "approved" if duplicates detected correctly, or describe issues</resume-signal>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `docker compose -f tests/docker/docker-compose.yml config` validates
- [ ] `./bootstrap.sh` brings up Immich and outputs API key
- [ ] `./seed-fixtures.sh` uploads all fixtures without errors
- [ ] `/api/duplicates` returns duplicate groups after seeding
- [ ] `./teardown.sh` cleanly removes containers and volumes
- [ ] Can run full cycle twice (bootstrap → seed → teardown → bootstrap → seed) without issues
</verification>

<success_criteria>
- Docker Compose configuration works on macOS (user's platform)
- Clean bootstrap completes in under 2 minutes
- All 34 fixture scenarios upload successfully
- Immich ML processes uploads and detects duplicates
- User can visually confirm duplicate detection in Immich UI
- Teardown removes all state for clean reruns
</success_criteria>

<output>
After completion, create `.planning/phases/06-synthetic-integration-tests/06-04-01-SUMMARY.md`:

# Phase 06 Plan 04-01: Docker Test Environment Summary

**[One-liner describing what was built]**

## Performance

- **Duration:** X min
- **Started:** [timestamp]
- **Completed:** [timestamp]
- **Tasks:** 4

## Accomplishments

- [Key outcomes]

## Files Created/Modified

- `tests/docker/docker-compose.yml` - Immich stack configuration
- `tests/docker/.env.test` - Environment variables
- `tests/docker/bootstrap.sh` - Start fresh instance
- `tests/docker/teardown.sh` - Clean shutdown
- `tests/docker/wait-for-ready.sh` - Health check polling
- `tests/docker/seed-fixtures.sh` - Upload test fixtures

## Decisions Made

[Key decisions and rationale]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase Readiness

Ready for 06-05: Integration Test Suite
</output>
