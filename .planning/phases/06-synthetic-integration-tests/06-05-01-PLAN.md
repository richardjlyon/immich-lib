---
phase: 06-synthetic-integration-tests
plan: 01
type: execute
---

<objective>
Create test infrastructure for integration tests against Docker Immich instance.

Purpose: Provide reusable setup/teardown, waiting, and assertion utilities for all integration test scenarios.
Output: Test harness module with Docker control, duplicate detection waiting, and manifest comparison helpers.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-synthetic-integration-tests/06-04-01-SUMMARY.md
@tests/fixtures/MANIFEST.md

**Prior context:**
- Docker environment created in 06-04: bootstrap.sh, seed-fixtures.sh, teardown.sh
- 34 fixture scenarios with manifest.json containing `expected_winner`
- Immich takes time after ML jobs complete before duplicates appear in API
- Need to poll `/api/duplicates` for actual duplicate data, not just job queue

**Key requirement from user:**
- Must wait for Immich to fully complete duplicate detection before tests run
- Don't rely on job queue status alone - poll for actual duplicate data
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration test harness module</name>
  <files>tests/integration/mod.rs, tests/integration/harness.rs</files>
  <action>
Create integration test infrastructure:

1. Create `tests/integration/` directory structure
2. `harness.rs` with:
   - `TestHarness` struct holding API key, base URL
   - `setup()` function that:
     - Runs `tests/docker/bootstrap.sh` via Command
     - Runs `tests/docker/seed-fixtures.sh` via Command
     - Reads API key from `tests/docker/.api_key`
     - Returns TestHarness instance
   - `teardown()` function that runs `tests/docker/teardown.sh`
   - `wait_for_duplicates()` function that:
     - Polls GET `/api/duplicates` every 5 seconds
     - Waits until response contains duplicate groups (non-empty array)
     - Times out after 120 seconds with clear error
     - Returns the duplicate groups on success
   - Uses `reqwest::blocking::Client` for simplicity in tests

3. Add `#[cfg(test)]` module in lib.rs or use tests/ directory

Note: Use `std::process::Command` for shell scripts, not tokio - keeps tests simpler.
Timeout for wait_for_duplicates should be generous (120s) since ML processing varies.
  </action>
  <verify>cargo test --no-run compiles without errors</verify>
  <done>TestHarness struct with setup/teardown/wait_for_duplicates compiles</done>
</task>

<task type="auto">
  <name>Task 2: Create manifest parser and result comparison utilities</name>
  <files>tests/integration/fixtures.rs, tests/integration/assertions.rs</files>
  <action>
Create test utilities:

1. `fixtures.rs`:
   - `Manifest` struct matching manifest.json structure:
     - scenario: String
     - description: String
     - images: Vec<String>
     - expected_winner: String
   - `load_manifest(scenario_code: &str) -> Manifest` function
     - Reads from `tests/fixtures/{code}/manifest.json`
     - Deserializes with serde_json
   - `list_scenarios() -> Vec<String>` function
     - Returns all scenario codes (w1, w2, ..., c1, ..., f1, ..., x1, ...)

2. `assertions.rs`:
   - `find_scenario_group(duplicates: &[DuplicateGroup], manifest: &Manifest) -> Option<&DuplicateGroup>`
     - Matches duplicate group by finding one where asset filenames match manifest.images
     - Use original_file_name from asset metadata
   - `assert_winner_matches(group: &ScoredDuplicateGroup, expected_winner: &str)`
     - Checks that group.winner.asset.original_file_name == expected_winner
     - Panics with clear message if mismatch

3. Update `tests/integration/mod.rs` to export all modules

Note: The DuplicateGroup and ScoredDuplicateGroup types come from immich_lib crate.
Use `immich_lib::models::*` for API types.
  </action>
  <verify>cargo test --no-run compiles, manifest parsing works</verify>
  <done>Can load manifests and compare winner selection results</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cargo test --no-run` succeeds (tests compile)
- [ ] `tests/integration/` directory structure exists
- [ ] Harness, fixtures, and assertions modules are importable
</verification>

<success_criteria>
- Test harness compiles and provides setup/teardown/wait functions
- Manifest parser correctly loads fixture metadata
- Assertion helpers ready for use in scenario tests
- All modules properly organized in tests/integration/
</success_criteria>

<output>
After completion, create `.planning/phases/06-synthetic-integration-tests/06-05-01-SUMMARY.md`
</output>
