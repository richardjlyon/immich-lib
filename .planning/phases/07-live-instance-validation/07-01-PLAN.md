---
phase: 07-live-instance-validation
plan: 01
type: execute
---

<objective>
Create validation runner script and execute full workflow against Docker test instance.

Purpose: Prove the complete analyze → execute pipeline works against a real Immich instance before considering production use.
Output: Execution report, backup files, and modified Immich database state ready for verification.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-live-instance-validation/07-CONTEXT.md
@.planning/phases/06-synthetic-integration-tests/06-05-03-SUMMARY.md

**Prior decisions affecting this phase:**
- Phase 6 pivoted to recorded fixture tests - execution workflow never ran against live Immich
- Docker infrastructure exists: bootstrap.sh, seed-fixtures.sh, teardown.sh
- 32 unique test fixtures with known winner/loser scenarios

**Gap being closed:**
Consolidation logic was unit-tested but never integration-tested against actual Immich API.

**Relevant source files:**
@tests/docker/bootstrap.sh
@tests/docker/seed-fixtures.sh
@tests/docker/docker-compose.yml
@src/executor.rs
@src/bin/immich-dupes.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation runner script</name>
  <files>tests/docker/run-validation.sh</files>
  <action>
Create a bash script that orchestrates the full validation workflow:

1. Source .env.test for API key and URL
2. Run bootstrap.sh (starts Docker, waits for ready)
3. Run seed-fixtures.sh (uploads test images, waits for duplicate detection)
4. Run `immich-dupes analyze` with output to tests/docker/validation-analysis.json
5. Create backup directory tests/docker/validation-backups/
6. Run `immich-dupes execute` with the analysis JSON
7. Save execution output to tests/docker/validation-report.txt

Script should:
- Use set -e for fail-fast behavior
- Print clear status messages at each step
- NOT run teardown (preserve state for verification)
- Export IMMICH_URL and IMMICH_API_KEY from .env.test
  </action>
  <verify>Script exists and is executable: `ls -la tests/docker/run-validation.sh`</verify>
  <done>run-validation.sh created with full workflow orchestration</done>
</task>

<task type="auto">
  <name>Task 2: Execute full validation workflow</name>
  <files>tests/docker/validation-analysis.json, tests/docker/validation-backups/, tests/docker/validation-report.txt</files>
  <action>
Run the validation script against Docker test instance:

1. Ensure Docker is running
2. Execute: `cd tests/docker && ./run-validation.sh`
3. Monitor output for any errors
4. Capture the execution report output

Expected outcomes:
- Analysis JSON created with scored duplicate groups
- Backup files downloaded to validation-backups/
- Losers deleted from Immich
- Winners remain with consolidated metadata
  </action>
  <verify>
- Analysis JSON exists: `ls -la tests/docker/validation-analysis.json`
- Backup directory has files: `ls tests/docker/validation-backups/ | wc -l`
- Report captured: `cat tests/docker/validation-report.txt`
  </verify>
  <done>Full workflow executed, backups created, report generated</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Executed full analyze → execute workflow against Docker Immich instance</what-built>
  <how-to-verify>
    1. Check validation-report.txt for execution summary
    2. Verify backup files exist in tests/docker/validation-backups/
    3. Check that the number of backups matches expected losers
    4. Spot-check a few backup files are valid images (can open them)
  </how-to-verify>
  <resume-signal>Type "approved" to continue to verification phase, or describe issues</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] run-validation.sh exists and is executable
- [ ] Validation workflow completed without errors
- [ ] Analysis JSON generated
- [ ] Backup files downloaded
- [ ] Execution report shows expected group counts
</verification>

<success_criteria>
- Validation script created and working
- Full workflow executed against Docker instance
- Backup files created for all losers
- Ready for end-state verification in 07-02
</success_criteria>

<output>
After completion, create `.planning/phases/07-live-instance-validation/07-01-SUMMARY.md`
</output>
