---
phase: 03-metadata-scoring
plan: 01
type: execute
---

<objective>
Implement metadata scoring algorithm that ranks assets by completeness and detects conflicts.

Purpose: Enable intelligent duplicate selection by scoring metadata richness and flagging cases where duplicates have conflicting (not just missing) metadata for manual review.
Output: Scoring module with MetadataScore, conflict detection, and DuplicateAnalysis that picks winners.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Prior decisions affecting this phase:**
- has_gps() requires BOTH latitude AND longitude
- has_camera_info() requires EITHER make OR model
- has_location() requires EITHER city OR country
- Flag metadata conflicts for manual review (user decision during planning)

**Relevant source files:**
@src/models/exif.rs - ExifInfo with has_*() helper methods
@src/models/asset.rs - AssetResponse with has_exif()
@src/models/duplicate.rs - DuplicateGroup containing Vec<AssetResponse>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create scoring module with MetadataScore</name>
  <files>src/scoring.rs, src/lib.rs</files>
  <action>
Create new module `src/scoring.rs` with:

1. `MetadataScore` struct with fields for each category:
   - gps: u32 (weight: 30 - most valuable, irreplaceable)
   - timezone: u32 (weight: 20 - important for correct timestamps)
   - camera_info: u32 (weight: 15 - useful provenance)
   - capture_time: u32 (weight: 15 - original timestamp)
   - lens_info: u32 (weight: 10 - nice to have)
   - location: u32 (weight: 10 - reverse-geocoded, derivable from GPS)
   - total: u32 (sum of weighted scores)

2. `impl MetadataScore`:
   - `fn from_asset(asset: &AssetResponse) -> Self` - Score an asset using ExifInfo helpers
   - Each category scores 0 (missing) or its weight value (present)
   - Total is sum of all category scores

3. Derive Debug, Clone, Default, PartialEq, Eq, PartialOrd, Ord on MetadataScore (Ord by total for easy comparison)

4. Add `pub mod scoring;` to lib.rs and re-export MetadataScore

Use the existing has_*() methods on ExifInfo - do NOT duplicate that logic.
  </action>
  <verify>
cargo build succeeds
cargo clippy -- -D warnings passes
cargo test (if any tests exist)
  </verify>
  <done>
MetadataScore struct exists with from_asset() that correctly scores assets using ExifInfo helpers. Weights: GPS=30, timezone=20, camera=15, capture_time=15, lens=10, location=10.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add conflict detection</name>
  <files>src/scoring.rs</files>
  <action>
Add conflict detection to scoring module:

1. `MetadataConflict` enum with variants:
   - Gps { values: Vec<(f64, f64)> } - Different GPS coordinates
   - Timezone { values: Vec<String> } - Different timezones
   - CameraInfo { values: Vec<String> } - Different make/model combinations
   - CaptureTime { values: Vec<String> } - Different original timestamps

2. `fn detect_conflicts(assets: &[AssetResponse]) -> Vec<MetadataConflict>`
   - For each metadata category, collect non-None values across all assets
   - If more than one UNIQUE value exists, it's a conflict
   - GPS conflict: coordinates differ by more than 0.0001 degrees (~11m) to allow for rounding
   - String conflicts: case-insensitive comparison, trim whitespace
   - Return list of detected conflicts (empty = no conflicts)

3. Derive Debug, Clone on MetadataConflict

Only detect conflicts for fields that MATTER for selection (GPS, timezone, camera, capture_time). Don't flag conflicts for description, rating, etc.
  </action>
  <verify>
cargo build succeeds
cargo clippy -- -D warnings passes
  </verify>
  <done>
detect_conflicts() identifies when duplicates have different values for the same metadata field. GPS uses ~11m threshold, strings use case-insensitive comparison.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create DuplicateAnalysis with winner selection</name>
  <files>src/scoring.rs</files>
  <action>
Add analysis result types:

1. `ScoredAsset` struct:
   - asset_id: String (clone from AssetResponse.id)
   - filename: String (clone from original_file_name)
   - score: MetadataScore
   - file_size: Option<u64> (from exif_info.file_size_in_byte)

2. `DuplicateAnalysis` struct:
   - duplicate_id: String
   - winner: ScoredAsset
   - losers: Vec<ScoredAsset>
   - conflicts: Vec<MetadataConflict>
   - needs_review: bool (true if conflicts non-empty)

3. `impl DuplicateAnalysis`:
   - `fn from_group(group: &DuplicateGroup) -> Self`
   - Score all assets in the group
   - Sort by score descending (highest wins)
   - On tie: prefer larger file size (Immich's behavior as tiebreaker)
   - On size tie: prefer first asset (stable sort)
   - Detect conflicts across all assets
   - Set needs_review = !conflicts.is_empty()

4. Derive Debug, Clone, Serialize on all new types (Serialize for JSON output in Phase 4)

Add `use serde::Serialize;` at top if not present.
  </action>
  <verify>
cargo build succeeds
cargo clippy -- -D warnings passes
cargo doc --no-deps succeeds (documentation builds)
  </verify>
  <done>
DuplicateAnalysis.from_group() scores all assets, picks winner by highest metadata score (file size as tiebreaker), detects conflicts, and sets needs_review flag.
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `cargo build` succeeds without errors
- [ ] `cargo clippy -- -D warnings` passes with no warnings
- [ ] `cargo doc --no-deps` builds documentation
- [ ] MetadataScore correctly uses ExifInfo helper methods
- [ ] Conflict detection handles GPS threshold and string normalization
- [ ] DuplicateAnalysis picks winner by score, uses file size as tiebreaker
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No clippy warnings
- Scoring module is ready for Phase 4 (analyze command with JSON output)
- needs_review flag correctly identifies groups with metadata conflicts
</success_criteria>

<output>
After completion, create `.planning/phases/03-metadata-scoring/03-01-SUMMARY.md`:

# Phase 3 Plan 01: Metadata Scoring Summary

**[Substantive one-liner]**

## Performance
- Duration: X min
- Tasks: 3

## Accomplishments
- [Key outcomes]

## Files Created/Modified
- `src/scoring.rs` - New scoring module
- `src/lib.rs` - Added scoring module export

## Decisions Made
- [Any decisions during implementation]

## Issues Encountered
- [Problems and resolutions, or "None"]

## Next Step
Phase complete, ready for Phase 4 (Analysis Stage)
</output>
